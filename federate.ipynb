{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-09T19:38:41.508093Z",
     "start_time": "2025-06-09T19:38:37.858692Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from federate import federated_train\n",
    "from utils import compute_metrics\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, Trainer, AutoModelForSequenceClassification, TrainingArguments\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/henry/anaconda3/envs/wse/lib/python3.13/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/henry/anaconda3/envs/wse/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "True\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T19:38:42.393610Z",
     "start_time": "2025-06-09T19:38:42.386452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ],
   "id": "368f5ed5e35757d3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T19:38:43.678007Z",
     "start_time": "2025-06-09T19:38:43.675541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MODEL_PATH = \"mBERT\" # I have donloaded distilled mBERT (hugggingface not available in China)\n",
    "MODEL_PATH = \"distilbert-base-multilingual-cased\"\n",
    "DATA_DIR = \"data\"\n"
   ],
   "id": "4d2d5df5ca40d57f",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T19:38:51.463957Z",
     "start_time": "2025-06-09T19:38:44.963037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_and_label(filepath, portion=0.1):\n",
    "    df = pd.read_csv(filepath).sample(frac=portion, random_state=42)\n",
    "    # Map stars to 0/1/2\n",
    "    def map_sentiment(stars):\n",
    "        if stars <= 2:\n",
    "            return 0  # negative\n",
    "        elif stars == 3:\n",
    "            return 1  # neutral\n",
    "        else:\n",
    "            return 2  # positive\n",
    "    df[\"label\"] = df[\"stars\"].apply(map_sentiment)\n",
    "    return Dataset.from_pandas(df[[\"review_body\", \"label\"]])\n",
    "\n",
    "train_ds = load_and_label(os.path.join(DATA_DIR, \"train.csv\"))\n",
    "val_ds = load_and_label(os.path.join(DATA_DIR, \"validation.csv\"))\n",
    "test_ds = load_and_label(os.path.join(DATA_DIR, \"test.csv\"))\n"
   ],
   "id": "3ae62232b5e3d948",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T19:42:19.725106Z",
     "start_time": "2025-06-09T19:41:55.384868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# ==== Tokenizer ====\n",
    "print(\"Loading tokenizer from local path...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(example[\"review_body\"], truncation=True, padding=\"max_length\",max_length=512)\n",
    "\n",
    "train_ds = train_ds.map(tokenize, batched=True)\n",
    "val_ds = val_ds.map(tokenize, batched=True)\n",
    "test_ds = test_ds.map(tokenize, batched=True)\n",
    "\n",
    "columns_to_keep = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "train_ds.set_format(type=\"torch\", columns=columns_to_keep)\n",
    "val_ds.set_format(type=\"torch\", columns=columns_to_keep)\n",
    "test_ds.set_format(type=\"torch\", columns=columns_to_keep)\n"
   ],
   "id": "77a773c73c6717e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from local path...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 120000/120000 [00:22<00:00, 5325.11 examples/s]\n",
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 5670.11 examples/s]\n",
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 5167.63 examples/s]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T19:42:21.444487Z",
     "start_time": "2025-06-09T19:42:21.290136Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "NUM_LABELS = 3\n",
    "\n",
    "# ==== Load Model ====\n",
    "print(\"Loading model from local path...\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=NUM_LABELS)\n",
    "\n",
    "# ==== Metrics ====\n",
    "\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "\n",
    "\n"
   ],
   "id": "60b671c215edc3fd",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from local path...\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T19:35:45.535100Z",
     "start_time": "2025-06-09T19:35:45.486151Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "52859a96411aed27",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'column_names'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m train_ds[\u001B[32m0\u001B[39m:\u001B[32m2\u001B[39m].column_names\n",
      "\u001B[31mAttributeError\u001B[39m: 'dict' object has no attribute 'column_names'"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-09T19:43:24.550610Z",
     "start_time": "2025-06-09T19:42:23.295557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n = len(train_ds)\n",
    "subset_size = n // 4\n",
    "\n",
    "client_datasets = [\n",
    "    train_ds.select(range(0, subset_size)),\n",
    "    train_ds.select(range(subset_size, 2 * subset_size)),\n",
    "    train_ds.select(range(2 * subset_size, 3 * subset_size)),\n",
    "    train_ds.select(range(3 * subset_size, 4 * subset_size)),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "federated_train(\n",
    "    base_model=model,\n",
    "    client_datasets=client_datasets,\n",
    "    val_ds=val_ds,\n",
    "    test_ds=test_ds,\n",
    "    client_weights=[0.25,0.25,0.25,0.25],\n",
    "    local_epochs=2,\n",
    "    global_rounds=3,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    learning_rate=5e-5,\n",
    "    device=device\n",
    ")"
   ],
   "id": "5de8f46c91921404",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Global Round 1/3 ---\n",
      " Evaluating global model before client updates...\n",
      "{'eval_loss': 1.0878561735153198, 'eval_model_preparation_time': 0.0009, 'eval_accuracy': 0.4023333333333333, 'eval_f1': 0.2577033526943883, 'eval_runtime': 56.7125, 'eval_samples_per_second': 52.898, 'eval_steps_per_second': 1.657}\n",
      " Global Evaluation (Round 1): {'eval_loss': 1.0878561735153198, 'eval_model_preparation_time': 0.0009, 'eval_accuracy': 0.4023333333333333, 'eval_f1': 0.2577033526943883, 'eval_runtime': 56.7125, 'eval_samples_per_second': 52.898, 'eval_steps_per_second': 1.657}\n",
      " Training on Client 1/4...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 7.91 GiB of which 62.94 MiB is free. Including non-PyTorch memory, this process has 7.19 GiB memory in use. Of the allocated memory 6.96 GiB is allocated by PyTorch, and 110.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOutOfMemoryError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 13\u001B[39m\n\u001B[32m      2\u001B[39m subset_size = n // \u001B[32m4\u001B[39m\n\u001B[32m      4\u001B[39m client_datasets = [\n\u001B[32m      5\u001B[39m     train_ds.select(\u001B[38;5;28mrange\u001B[39m(\u001B[32m0\u001B[39m, subset_size)),\n\u001B[32m      6\u001B[39m     train_ds.select(\u001B[38;5;28mrange\u001B[39m(subset_size, \u001B[32m2\u001B[39m * subset_size)),\n\u001B[32m      7\u001B[39m     train_ds.select(\u001B[38;5;28mrange\u001B[39m(\u001B[32m2\u001B[39m * subset_size, \u001B[32m3\u001B[39m * subset_size)),\n\u001B[32m      8\u001B[39m     train_ds.select(\u001B[38;5;28mrange\u001B[39m(\u001B[32m3\u001B[39m * subset_size, \u001B[32m4\u001B[39m * subset_size)),\n\u001B[32m      9\u001B[39m ]\n\u001B[32m---> \u001B[39m\u001B[32m13\u001B[39m federated_train(\n\u001B[32m     14\u001B[39m     base_model=model,\n\u001B[32m     15\u001B[39m     client_datasets=client_datasets,\n\u001B[32m     16\u001B[39m     val_ds=val_ds,\n\u001B[32m     17\u001B[39m     test_ds=test_ds,\n\u001B[32m     18\u001B[39m     client_weights=[\u001B[32m0.25\u001B[39m,\u001B[32m0.25\u001B[39m,\u001B[32m0.25\u001B[39m,\u001B[32m0.25\u001B[39m],\n\u001B[32m     19\u001B[39m     local_epochs=\u001B[32m2\u001B[39m,\n\u001B[32m     20\u001B[39m     global_rounds=\u001B[32m3\u001B[39m,\n\u001B[32m     21\u001B[39m     batch_size=BATCH_SIZE,\n\u001B[32m     22\u001B[39m     learning_rate=\u001B[32m5e-5\u001B[39m,\n\u001B[32m     23\u001B[39m     device=device\n\u001B[32m     24\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/school/wse-project/federate.py:176\u001B[39m, in \u001B[36mfederated_train\u001B[39m\u001B[34m(base_model, client_datasets, val_ds, test_ds, client_weights, local_epochs, global_rounds, batch_size, learning_rate, device)\u001B[39m\n\u001B[32m    167\u001B[39m     trainer = Trainer(\n\u001B[32m    168\u001B[39m         model=client_model,\n\u001B[32m    169\u001B[39m         args=training_args,\n\u001B[32m   (...)\u001B[39m\u001B[32m    172\u001B[39m         compute_metrics=compute_metrics,\n\u001B[32m    173\u001B[39m     )\n\u001B[32m    175\u001B[39m \u001B[38;5;66;03m# Perform local training for the client\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m176\u001B[39m trainer.train()\n\u001B[32m    177\u001B[39m \u001B[38;5;66;03m# Move client model to CPU before appending to avoid potential device issues during aggregation\u001B[39;00m\n\u001B[32m    178\u001B[39m client_models.append(client_model.cpu())\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/transformers/trainer.py:2240\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2238\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2239\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2240\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m inner_training_loop(\n\u001B[32m   2241\u001B[39m         args=args,\n\u001B[32m   2242\u001B[39m         resume_from_checkpoint=resume_from_checkpoint,\n\u001B[32m   2243\u001B[39m         trial=trial,\n\u001B[32m   2244\u001B[39m         ignore_keys_for_eval=ignore_keys_for_eval,\n\u001B[32m   2245\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/transformers/trainer.py:2555\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2548\u001B[39m context = (\n\u001B[32m   2549\u001B[39m     functools.partial(\u001B[38;5;28mself\u001B[39m.accelerator.no_sync, model=model)\n\u001B[32m   2550\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m i != \u001B[38;5;28mlen\u001B[39m(batch_samples) - \u001B[32m1\u001B[39m\n\u001B[32m   2551\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001B[32m   2552\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m contextlib.nullcontext\n\u001B[32m   2553\u001B[39m )\n\u001B[32m   2554\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m-> \u001B[39m\u001B[32m2555\u001B[39m     tr_loss_step = \u001B[38;5;28mself\u001B[39m.training_step(model, inputs, num_items_in_batch)\n\u001B[32m   2557\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2558\u001B[39m     args.logging_nan_inf_filter\n\u001B[32m   2559\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[32m   2560\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (torch.isnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch.isinf(tr_loss_step))\n\u001B[32m   2561\u001B[39m ):\n\u001B[32m   2562\u001B[39m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[32m   2563\u001B[39m     tr_loss = tr_loss + tr_loss / (\u001B[32m1\u001B[39m + \u001B[38;5;28mself\u001B[39m.state.global_step - \u001B[38;5;28mself\u001B[39m._globalstep_last_logged)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/transformers/trainer.py:3745\u001B[39m, in \u001B[36mTrainer.training_step\u001B[39m\u001B[34m(self, model, inputs, num_items_in_batch)\u001B[39m\n\u001B[32m   3742\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb.reduce_mean().detach().to(\u001B[38;5;28mself\u001B[39m.args.device)\n\u001B[32m   3744\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.compute_loss_context_manager():\n\u001B[32m-> \u001B[39m\u001B[32m3745\u001B[39m     loss = \u001B[38;5;28mself\u001B[39m.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n\u001B[32m   3747\u001B[39m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[32m   3748\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   3749\u001B[39m     \u001B[38;5;28mself\u001B[39m.args.torch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   3750\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.state.global_step % \u001B[38;5;28mself\u001B[39m.args.torch_empty_cache_steps == \u001B[32m0\u001B[39m\n\u001B[32m   3751\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/transformers/trainer.py:3810\u001B[39m, in \u001B[36mTrainer.compute_loss\u001B[39m\u001B[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[39m\n\u001B[32m   3808\u001B[39m         loss_kwargs[\u001B[33m\"\u001B[39m\u001B[33mnum_items_in_batch\u001B[39m\u001B[33m\"\u001B[39m] = num_items_in_batch\n\u001B[32m   3809\u001B[39m     inputs = {**inputs, **loss_kwargs}\n\u001B[32m-> \u001B[39m\u001B[32m3810\u001B[39m outputs = model(**inputs)\n\u001B[32m   3811\u001B[39m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[32m   3812\u001B[39m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[32m   3813\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.past_index >= \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/transformers/models/distilbert/modeling_distilbert.py:920\u001B[39m, in \u001B[36mDistilBertForSequenceClassification.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m    912\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    913\u001B[39m \u001B[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[32m    914\u001B[39m \u001B[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[32m    915\u001B[39m \u001B[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[32m    916\u001B[39m \u001B[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[32m    917\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    918\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m--> \u001B[39m\u001B[32m920\u001B[39m distilbert_output = \u001B[38;5;28mself\u001B[39m.distilbert(\n\u001B[32m    921\u001B[39m     input_ids=input_ids,\n\u001B[32m    922\u001B[39m     attention_mask=attention_mask,\n\u001B[32m    923\u001B[39m     head_mask=head_mask,\n\u001B[32m    924\u001B[39m     inputs_embeds=inputs_embeds,\n\u001B[32m    925\u001B[39m     output_attentions=output_attentions,\n\u001B[32m    926\u001B[39m     output_hidden_states=output_hidden_states,\n\u001B[32m    927\u001B[39m     return_dict=return_dict,\n\u001B[32m    928\u001B[39m )\n\u001B[32m    929\u001B[39m hidden_state = distilbert_output[\u001B[32m0\u001B[39m]  \u001B[38;5;66;03m# (bs, seq_len, dim)\u001B[39;00m\n\u001B[32m    930\u001B[39m pooled_output = hidden_state[:, \u001B[32m0\u001B[39m]  \u001B[38;5;66;03m# (bs, dim)\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/transformers/models/distilbert/modeling_distilbert.py:739\u001B[39m, in \u001B[36mDistilBertModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m    734\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._use_sdpa \u001B[38;5;129;01mand\u001B[39;00m head_mask_is_none \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m output_attentions:\n\u001B[32m    735\u001B[39m         attention_mask = _prepare_4d_attention_mask_for_sdpa(\n\u001B[32m    736\u001B[39m             attention_mask, embeddings.dtype, tgt_len=input_shape[\u001B[32m1\u001B[39m]\n\u001B[32m    737\u001B[39m         )\n\u001B[32m--> \u001B[39m\u001B[32m739\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.transformer(\n\u001B[32m    740\u001B[39m     x=embeddings,\n\u001B[32m    741\u001B[39m     attn_mask=attention_mask,\n\u001B[32m    742\u001B[39m     head_mask=head_mask,\n\u001B[32m    743\u001B[39m     output_attentions=output_attentions,\n\u001B[32m    744\u001B[39m     output_hidden_states=output_hidden_states,\n\u001B[32m    745\u001B[39m     return_dict=return_dict,\n\u001B[32m    746\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/transformers/models/distilbert/modeling_distilbert.py:544\u001B[39m, in \u001B[36mTransformer.forward\u001B[39m\u001B[34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m    536\u001B[39m     layer_outputs = \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(\n\u001B[32m    537\u001B[39m         layer_module.\u001B[34m__call__\u001B[39m,\n\u001B[32m    538\u001B[39m         hidden_state,\n\u001B[32m   (...)\u001B[39m\u001B[32m    541\u001B[39m         output_attentions,\n\u001B[32m    542\u001B[39m     )\n\u001B[32m    543\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m544\u001B[39m     layer_outputs = layer_module(\n\u001B[32m    545\u001B[39m         hidden_state,\n\u001B[32m    546\u001B[39m         attn_mask,\n\u001B[32m    547\u001B[39m         head_mask[i],\n\u001B[32m    548\u001B[39m         output_attentions,\n\u001B[32m    549\u001B[39m     )\n\u001B[32m    551\u001B[39m hidden_state = layer_outputs[-\u001B[32m1\u001B[39m]\n\u001B[32m    553\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/transformers/models/distilbert/modeling_distilbert.py:488\u001B[39m, in \u001B[36mTransformerBlock.forward\u001B[39m\u001B[34m(self, x, attn_mask, head_mask, output_attentions)\u001B[39m\n\u001B[32m    485\u001B[39m sa_output = \u001B[38;5;28mself\u001B[39m.sa_layer_norm(sa_output + x)  \u001B[38;5;66;03m# (bs, seq_length, dim)\u001B[39;00m\n\u001B[32m    487\u001B[39m \u001B[38;5;66;03m# Feed Forward Network\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m488\u001B[39m ffn_output = \u001B[38;5;28mself\u001B[39m.ffn(sa_output)  \u001B[38;5;66;03m# (bs, seq_length, dim)\u001B[39;00m\n\u001B[32m    489\u001B[39m ffn_output: torch.Tensor = \u001B[38;5;28mself\u001B[39m.output_layer_norm(ffn_output + sa_output)  \u001B[38;5;66;03m# (bs, seq_length, dim)\u001B[39;00m\n\u001B[32m    491\u001B[39m output = (ffn_output,)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/transformers/models/distilbert/modeling_distilbert.py:422\u001B[39m, in \u001B[36mFFN.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    421\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: torch.Tensor) -> torch.Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m422\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m apply_chunking_to_forward(\u001B[38;5;28mself\u001B[39m.ff_chunk, \u001B[38;5;28mself\u001B[39m.chunk_size_feed_forward, \u001B[38;5;28mself\u001B[39m.seq_len_dim, \u001B[38;5;28minput\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/transformers/pytorch_utils.py:253\u001B[39m, in \u001B[36mapply_chunking_to_forward\u001B[39m\u001B[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[39m\n\u001B[32m    250\u001B[39m     \u001B[38;5;66;03m# concatenate output at same dimension\u001B[39;00m\n\u001B[32m    251\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001B[32m--> \u001B[39m\u001B[32m253\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m forward_fn(*input_tensors)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/transformers/models/distilbert/modeling_distilbert.py:426\u001B[39m, in \u001B[36mFFN.ff_chunk\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    424\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mff_chunk\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: torch.Tensor) -> torch.Tensor:\n\u001B[32m    425\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.lin1(\u001B[38;5;28minput\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m426\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.activation(x)\n\u001B[32m    427\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.lin2(x)\n\u001B[32m    428\u001B[39m     x = \u001B[38;5;28mself\u001B[39m.dropout(x)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._call_impl(*args, **kwargs)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(*args, **kwargs)\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/anaconda3/envs/wse/lib/python3.13/site-packages/transformers/activations.py:69\u001B[39m, in \u001B[36mGELUActivation.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m     68\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m---> \u001B[39m\u001B[32m69\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.act(\u001B[38;5;28minput\u001B[39m)\n",
      "\u001B[31mOutOfMemoryError\u001B[39m: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 7.91 GiB of which 62.94 MiB is free. Including non-PyTorch memory, this process has 7.19 GiB memory in use. Of the allocated memory 6.96 GiB is allocated by PyTorch, and 110.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
